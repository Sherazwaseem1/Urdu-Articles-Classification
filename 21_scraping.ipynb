{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from BeautifulSoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "    def get_Geo_articles(self, max_pages=7):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://urdu.geo.tv/'\n",
    "        categories = ['entertainment', 'business', 'sports', 'science-technology', 'world']   # saqafat is entertainment category\n",
    "        \n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            # for page in range(1, max_pages + 1):\n",
    "                \n",
    "                print(f\"Scraping of category '{category}'...\")\n",
    "                \n",
    "                url = f\"{base_url}/category/{category}\"                \n",
    "                response = requests.get(url)\n",
    "                if response.url != url:  # Check if redirection occurred\n",
    "                    print(f\"Redirected from {url} to {response.url}\")\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find_all('li', class_='border-box')\n",
    "                print(f\"\\t--> Found {len(cards)} articles.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        \n",
    "                        div = card.find('a', class_='open-section')  # You are directly searching for the <a> tag\n",
    "\n",
    "                        if not div:\n",
    "                            continue\n",
    "                        \n",
    "                        # Extract Article Title\n",
    "                        headline = card.find('a')['title'].strip().replace('\\xa0', ' ')\n",
    "                        \n",
    "                        # Extract Article Link (href)\n",
    "                        link = div['href']\n",
    "                        \n",
    "                        if not link:\n",
    "                            continue\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('div', class_='content-area').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        geo_df['id'].append(self.id)\n",
    "                        geo_df['title'].append(headline)\n",
    "                        geo_df['link'].append(link)\n",
    "                        geo_df['gold_label'].append(category.replace('science','science-technology'))\n",
    "                        geo_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on {category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles of '{category}'.\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        return pd.DataFrame(geo_df)\n",
    "        \n",
    "    def get_Jang_articles(self, max_pages=7):\n",
    "        \n",
    "        Jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://jang.com.pk'\n",
    "        categories = ['entertainment', 'business', 'sports', 'health-science',\n",
    "                      'world']        \n",
    "         # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            \n",
    "            url = f\"{base_url}/category/latest-news/{category}\"\n",
    "                        \n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Finding article cards\n",
    "            cards = soup.find_all('div', class_='main-pic')\n",
    "            print(f\"\\t--> Found {len(cards)} articles on  of '{category}'.\")\n",
    "\n",
    "            success_count = 0\n",
    "\n",
    "            for card in cards:\n",
    "                try:\n",
    "                    \n",
    "                    div = card.find('div', class_='main-pic')\n",
    "\n",
    "                    # For the headline, get text from the 'a' tag\n",
    "                    headline = card.find('a')['title'].strip().replace('\\xa0', ' ')\n",
    "\n",
    "                    # For the article link\n",
    "                    link = card.find('a')['href']\n",
    "                    \n",
    "                    # Requesting the content from each article's link\n",
    "                    article_response = requests.get(link)\n",
    "                    article_response.raise_for_status()\n",
    "                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                    # Content arranged in paras inside <span> tags\n",
    "                    paras = content_soup.find('div',class_='detail_view_content').find_all('p')\n",
    "\n",
    "                    combined_text = \" \".join(\n",
    "                    p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                    for p in paras if p.get_text(strip=True)\n",
    "                    )\n",
    "\n",
    "                    # Storing data\n",
    "                    Jang_df['id'].append(self.id)\n",
    "                    Jang_df['title'].append(headline)\n",
    "                    Jang_df['link'].append(link)\n",
    "                    Jang_df['gold_label'].append(category.replace('health-science','science-technology'))\n",
    "                    Jang_df['content'].append(combined_text)\n",
    "\n",
    "                    # Increment ID and success count\n",
    "                    self.id += 1\n",
    "                    success_count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape an article on  of '{category}': {e}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles from of '{category}'.\")\n",
    "\n",
    "        return pd.DataFrame(Jang_df)\n",
    "        \n",
    "    def get_ARY_articles(self, max_pages=7):\n",
    "        \n",
    "        ARY_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        \n",
    "        \n",
    "        base_url = 'https://urdu.arynews.tv' \n",
    "        categories = ['fun-o-sakafat', 'کاروباری-خبریں', 'sports-2', 'سائنس-اور-ٹیکنالوجی', 'international-2']   # saqafat is entertainment category\n",
    "        \n",
    "        for category in categories:\n",
    "            \n",
    "                count = 0\n",
    "\n",
    "                headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:118.0) Gecko/20100101 Firefox/118.0 Edg/118.0\",\n",
    "                    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "                    \"Referer\": \"https://urdu.arynews.tv\"\n",
    "                }\n",
    "\n",
    "                url = f\"{base_url}/category/{category}\"\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                print(\"Getting the cards\")\n",
    "                \n",
    "                cards = soup.find_all('div', class_='td-module-meta-info')\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    \n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        h3 = card.find('h3', class_='entry-title td-module-title')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = h3.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = h3.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        \n",
    "                        article_response = requests.get(link, headers=headers)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find_all('p')\n",
    "                                                \n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "                        \n",
    "                        print(combined_text)\n",
    "\n",
    "                        # Storing data\n",
    "                        ARY_df['id'].append(self.id)\n",
    "                        ARY_df['title'].append(headline)\n",
    "                        ARY_df['link'].append(link)\n",
    "                        ARY_df['gold_label'].append(category.replace('fun-o-sakafat','entertainment').replace('سائنس-اور-ٹیکنالوجی','science-technology').replace('international-2','world').replace('کاروباری-خبریں','business').replace('sports-2','sports'))\n",
    "                        ARY_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from '{category}'.\")\n",
    "          \n",
    "\n",
    "        return pd.DataFrame(ARY_df)   \n",
    "        \n",
    "    def get_Duniya_articles(self, max_pages=7):\n",
    "        dawn_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.dawn.com'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "    def get_express_articles(self, max_pages=8):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 1 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'saqafat'.\n",
      "Scraping page 2 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 2 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'saqafat'.\n",
      "Scraping page 3 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 3 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'saqafat'.\n",
      "Scraping page 4 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 4 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'saqafat'.\n",
      "Scraping page 5 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 5 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'saqafat'.\n",
      "Scraping page 6 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 6 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'saqafat'.\n",
      "Scraping page 7 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 7 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'saqafat'.\n",
      "Scraping page 8 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 8 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'saqafat'.\n",
      "\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 10 articles on page 1 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 10 articles on page 2 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 10 articles on page 3 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 10 articles on page 4 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 10 articles on page 5 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 10 articles on page 6 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 10 articles on page 7 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'business'.\n",
      "Scraping page 8 of category 'business'...\n",
      "\t--> Found 10 articles on page 8 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'business'.\n",
      "\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 10 articles on page 1 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'sports'.\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 10 articles on page 2 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'sports'.\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 10 articles on page 3 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'sports'.\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 10 articles on page 4 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'sports'.\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 10 articles on page 5 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'sports'.\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 10 articles on page 6 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'sports'.\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 10 articles on page 7 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'sports'.\n",
      "Scraping page 8 of category 'sports'...\n",
      "\t--> Found 10 articles on page 8 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'sports'.\n",
      "\n",
      "Scraping page 1 of category 'science'...\n",
      "\t--> Found 10 articles on page 1 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'science'.\n",
      "Scraping page 2 of category 'science'...\n",
      "\t--> Found 10 articles on page 2 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'science'.\n",
      "Scraping page 3 of category 'science'...\n",
      "\t--> Found 10 articles on page 3 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'science'.\n",
      "Scraping page 4 of category 'science'...\n",
      "\t--> Found 10 articles on page 4 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'science'.\n",
      "Scraping page 5 of category 'science'...\n",
      "\t--> Found 10 articles on page 5 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'science'.\n",
      "Scraping page 6 of category 'science'...\n",
      "\t--> Found 10 articles on page 6 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'science'.\n",
      "Scraping page 7 of category 'science'...\n",
      "\t--> Found 10 articles on page 7 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'science'.\n",
      "Scraping page 8 of category 'science'...\n",
      "\t--> Found 10 articles on page 8 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'science'.\n",
      "\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 10 articles on page 1 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'world'.\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 10 articles on page 2 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'world'.\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 10 articles on page 3 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'world'.\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 10 articles on page 4 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'world'.\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 10 articles on page 5 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'world'.\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 10 articles on page 6 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'world'.\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 10 articles on page 7 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'world'.\n",
      "Scraping page 8 of category 'world'...\n",
      "\t--> Found 10 articles on page 8 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 8 of 'world'.\n",
      "\n",
      "Scraping of category 'entertainment'...\n",
      "\t--> Found 60 articles.\n",
      "\t--> Successfully scraped 60 articles of 'entertainment'.\n",
      "Scraping of category 'business'...\n",
      "\t--> Found 60 articles.\n",
      "\t--> Successfully scraped 60 articles of 'business'.\n",
      "Scraping of category 'sports'...\n",
      "\t--> Found 60 articles.\n",
      "\t--> Successfully scraped 60 articles of 'sports'.\n",
      "Scraping of category 'science-technology'...\n",
      "\t--> Found 60 articles.\n",
      "\t--> Successfully scraped 60 articles of 'science-technology'.\n",
      "Scraping of category 'world'...\n",
      "\t--> Found 60 articles.\n",
      "\t--> Successfully scraped 60 articles of 'world'.\n",
      "\t--> Found 103 articles on  of 'entertainment'.\n",
      "\t--> Failed to scrape an article on  of 'entertainment': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'entertainment': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'entertainment': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'entertainment': 'NoneType' object is not subscriptable\n",
      "\t--> Successfully scraped 99 articles from of 'entertainment'.\n",
      "\t--> Found 101 articles on  of 'business'.\n",
      "\t--> Failed to scrape an article on  of 'business': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'business': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'business': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'business': 'NoneType' object is not subscriptable\n",
      "\t--> Successfully scraped 97 articles from of 'business'.\n",
      "\t--> Found 103 articles on  of 'sports'.\n",
      "\t--> Failed to scrape an article on  of 'sports': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'sports': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'sports': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'sports': 'NoneType' object is not subscriptable\n",
      "\t--> Successfully scraped 99 articles from of 'sports'.\n",
      "\t--> Found 103 articles on  of 'health-science'.\n",
      "\t--> Failed to scrape an article on  of 'health-science': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'health-science': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'health-science': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'health-science': 'NoneType' object is not subscriptable\n",
      "\t--> Successfully scraped 99 articles from of 'health-science'.\n",
      "\t--> Found 104 articles on  of 'world'.\n",
      "\t--> Failed to scrape an article on  of 'world': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'world': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'world': 'NoneType' object is not subscriptable\n",
      "\t--> Failed to scrape an article on  of 'world': 'NoneType' object is not subscriptable\n",
      "\t--> Successfully scraped 100 articles from of 'world'.\n"
     ]
    }
   ],
   "source": [
    "express_df = scraper.get_express_articles() # Confirm Correct\n",
    "geo_df = scraper.get_Geo_articles() # Looks Correct\n",
    "Jang_df = scraper.get_Jang_articles() # Looks correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 3 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 5)\n",
      "(300, 5)\n",
      "(494, 5)\n"
     ]
    }
   ],
   "source": [
    "print(express_df.shape) # 300 something\n",
    "print(geo_df.shape) # 160\n",
    "# print(ARY_df.shape) # 50\n",
    "print(Jang_df.shape)   # 297\n",
    "\n",
    "combined_df = pd.concat([express_df, geo_df, Jang_df], ignore_index=True)\n",
    "\n",
    "combined_df.drop_duplicates(subset='link', keep='first', inplace=True)\n",
    "\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "csv_filename = 'news_articles_combined.csv'\n",
    "combined_df.to_csv(csv_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
